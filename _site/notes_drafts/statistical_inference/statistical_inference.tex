\input{./../note_style}

\title{\Huge \color{C1}  Statistical inference}
\author{Ethan Levien}
\date{April 2022}

\begin{document}

\maketitle

\tableofcontents




\section{Estimators}
Previously, we say that if we survey $N$ people in a population for which each individual has a chance $q$ of answering YES (for example, because they are an unbiased sample from a larger population for which $q$ is determined). 

{\bf Question}: If we find that $Y$ people respond YES, what is our best estimate of the true value of $q$?

 This should be $Y/N$, since for an individual response $x_i =0,1$, $\E[x_i] = q$ and 
 \begin{equation}
 \bar{x}_i = Y/N \approx \E[x_i] = q. 
 \end{equation}
 At the same time, we expect that for any particular sample of the population 
\begin{equation}
\frac{Y}{N} \ne q
\end{equation}
since there is always a chance that we happen to sample more or less people who answer YES.  In this case we call $Y/N$ an {\dfn estimator} of $q$, and write $\hat{q} = Y/N$. 


Can we quantify how much this two quantities will typically differ? This is related to the idea of uncertainty quantification, a central topic in statistics. The idea is that we want to quantify how confident we are in something we've inferred. 





\subsection{Standard errors}
In classical statistics, we measure accuracy using the standard error, denoted ${\rm se}(\hat{q})$.  The standard error is the standard deviation of $\hat{q}$ taken over different replications of our experiment. If we are, say, flipping a coin and tallying the result to obtain $Y$, it is clear what this means. If $Y$ is the vote share from a one-off election, then is becomes a bit puzzling to think about replicating the experiment. Fortunately, this is a philosophical problem, not a mathematical one. Mathematically, we can always define ${\rm se}(\hat{q})$ \emph{within the context of our model} as 
\begin{equation}
{\rm se}(\hat{\theta}) = \sqrt{{\rm var}(\hat{\theta}(Y))}
\end{equation}
where the variance is taken over the distribution of $Y$. That is, we use the probability distribution $P(Y)$ to compute this variance. 
Roughly speaking, if we performed many experiments and measured $\hat{q}$, the measurements will typically differ by ${\rm se}(\hat{q})$. 

\begin{exercise}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=jmB0Mvksc6B_&line=1&uniqifier=1}{Standard errors for binomial model}
\end{exercise} 


\begin{exercise}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=crvRJXiBe3r0&line=4&uniqifier=1}{Experimental design}
\end{exercise} 

\subsection{Bias and consistency}


There must be some properties we would like the estimator to have. At a minimum, it should be in some way informed by the data (we wouldn't want to set $\hat{q} = 1/2$ based solely on our intuition). We express this with the assumptions that: The more data we have (e.g. the larger $N$) the closer we expect $\hat{q}$ to be to the true value. To make this precise, we define an estimator $\hat{q}$ to be {\dfn consistent} if $\hat{q}$ converges to $q$ as $n$ grows. But what does convergence mean when we are dealing with random variables? This turns out to be technical, as there are different things this can mean. For our purposes, we can understand converge as 
\begin{equation}
{\rm se}(\hat{\theta})  \to 0 \,\,\,\text{ as }\,\, N\to \infty. 
\end{equation}


\begin{example}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=Le6SGPp9i161&line=3&uniqifier=1}{Understanding consistency}
\end{example} 


To better understand the notation of consistence, let's consider two rather silly ways to estimate $q$. Let $\hat{q}_1$ and $\hat{q}_2$ be two other estimators of $q$ defined by 
\begin{align}
\hat{q}_{1} &= \frac{k}{n} + \frac{1}{n}\\
\hat{q}_{2} &= y_i
\end{align}

\begin{exercise}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=Le6SGPp9i161&line=3&uniqifier=1}{Understanding consistency}
\end{exercise} 


This exercise demonstrates that consistency is not the only property we look for in an estimator, since $\hat{q}_1$ seems inferior to $\hat{q}_{\rm MLE}$. To this end, we say that an estimator is {\dfn biased} if an estimator is, on average, equal to the value of $q$ used to generate the data. In other words, if we run many simulations, or take many different samples from a population and compute the estimator, then we should get the true value of $q$. 

\begin{exercise}
 \href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=US27cD1JgXn_&line=3&uniqifier=1}{Understanding bias}
\end{exercise} 


\subsection{MLE}



Recall that the probability distribution for the binomial distribution is 
\begin{equation}\label{eq:binomial-pdf}
p(Y) = {n \choose Y}q^Y(1-q)^{n-Y}
\end{equation}
In statistics, we sometimes call this the {\dfn likelihood}. More generally, the likelihood is defined as the probability we say a data set as a function of the parameters. 

\subsection{Maximum Likelihood}
Equation \eqref{eq:binomial-pdf} tells us how likely it is to observe $k$ YES among $n$ people surveyed. Then, it seems reasonable that this number should not be very small, since that would mean our survey results are an anomaly. More generally, the larger $\Prob(Y|q)$ is the more likelihood our results are. This suggests one a way to estimate determine $q$: We can take as our estimate $\hat{q}$ the value which makes $\Prob(Y|q)$ largest. In other words, we are finding the value of $q$ which makes the data the most likely, and we will call this the {\dfn  maximum likelihood estimate}.

You can do this using calculus (if you know how, I suggest you give it a try) to determine that the value of $q$ which makes \eqref{eq:binomial-pdf} largest is \begin{equation}
\hat{q}_{\rm MLE} = \frac{Y}{n}
\end{equation}
MLEs are very useful, but as we learn later on, they are only one type of estimator. 




\section{Inference for a Normal distribution}

Suppose have $y_1,\dots,y_n$ from a variable which follows a Normal distribution, that is 
\begin{equation}
y_i \sim {\rm Normal}(\mu,\sigma)
\end{equation}
What is our best estimate of $\mu$ and $\sigma$? 

from a Normal distribution with mean and variance $\mu$ and $\sigma$, the MLE estimators are 
\begin{equation}
\hat{\mu}_{\rm MLE} = \frac{1}{n}\sum y_i
\end{equation}
and 
\begin{equation}
\hat{\sigma}_{\rm MLE} = \sqrt{\frac{1}{n-1}\sum ( y_i - \hat{\mu})^2}
\end{equation}


\begin{exercise}
\href{}{Consistency of MLE for Normal distribution} 
\end{exercise} 



\section{Hypothesis testing}
In statistics, we might infer parameters, such as $q$, not because we are interested in specific values, but rather because we would like to use them to make a decision. For example, whether a candidate drug is worth moving to the next step in clinical trials. This problem is often framed in terms of {\dfn hypothesis testing}, in which we assign a probability to a particular hypothesis or its converse. 

Suppose for example, that we conduct a clinical trial as follows. Two groups of $N$ people, the control group (C) and treatment group (T), are randomly selected from the population. People in T are given a drug which reduces their blood pressure and those control group are given a placebo. We can model the distribution of blood pressure before and after treatment as 
\begin{equation}
Y_C \sim {\rm Normal}(\mu_C,\sigma)
\end{equation}
and 
\begin{equation}
Y_T \sim {\rm Normal}(\mu_T,\sigma)
\end{equation}
This means or model for the sample distributions of the means are 
\begin{equation}
\hat{\mu}_T \sim {\rm Normal}(\mu_T,\sigma/\sqrt{N})
\end{equation}
If  $\Delta \mu = \mu_T-\mu_C$ the sample distribution of $\Delta \mu$ is also Normal. We are ultimately interested in moving the drug to the next phase of a clinical trial, which means determining if $\Delta \mu =0$. 


The approach to this problem of frequentist statistics is to ask: How likely is it that we would observe an effect at last as large as we did if the null hypothesis was true. The $p$-value in this context is the chance we  
\begin{equation}
p_v = P(\Delta \hat{\mu}/{\rm std}(\Delta \hat{\mu}) > \Delta \hat{\mu}|\mu_C = \mu_T)
\end{equation}
If the $p$-value is very small, then it is highly unlikely we would have observed what we did when the null hypothesis was true. In this case, we can REJECT the null hypothesis as false. Usually some threshold is set for this, and if the $p_v$ is below that threshdold we say our result in statistically significant. 

\begin{exercise}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=f3odqYsslqkj&line=3&uniqifier=1}{$p$-values}
\end{exercise} 

We can understand statistical significance in terms of statdard errors as well. 


\subsection{Problems with $p$-values, hypothesis testing and statistical significance}
Despite the widespread use of $p$-values, classical hypothesis testing and statistical significance, these concepts have some problems. This does not mean they are not useful, rather it is important to understand how they might be applied in appropriately in practice. 

First, typically the null hypothesis is never true, that is it is never the case that two subpopulations are exactly equal -- that is, that there is no effect. If we have enough data, we can alsmost always rule out the null hypothesis.  


\begin{exercise}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=f3odqYsslqkj&line=3&uniqifier=1}{Behavior of $p$-values in $N$ and effect size.  }
\end{exercise} 

A major issue in who statistical significance is used in practice, is that is can create a selection bais in the published liturature, where effects sizes are almost always over estimates. 
\begin{exercise}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=f3odqYsslqkj&line=3&uniqifier=1}{Bias in the liturature }
\end{exercise} 

Finally, a philosophical problem with statistical signifiance is that te difference between statistically signficant. 
\begin{exercise}
\href{https://colab.research.google.com/drive/1QarJhwPmSqCTQ-HwU_lXCUX6uvdhLdrM#scrollTo=f3odqYsslqkj&line=3&uniqifier=1}{Problems with statistical signficance }
\end{exercise} 








\end{document}





















\end{document}